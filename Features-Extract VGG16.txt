Script started on Tuesday 11 September 2018 09:05:48 PM IST
]0;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master/VGG16/VGG16[01;32msivamurugan@sivamurugan-PC[00m:[01;34m~/deep-learning-models-master/VGG16/VGG16[00m$ python2 Features-Extract.py 
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2018-09-11 21:06:09.588567: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: FMA
Found 83484 images belonging to 4 classes.
Features shape (100, 7, 7, 512)
Batch:1 Sample:100Batch:2 Sample:200Batch:3 Sample:300Batch:4 Sample:400Batch:5 Sample:500Batch:6 Sample:600Batch:7 Sample:700Batch:8 Sample:800Batch:9 Sample:900Batch:10 Sample:1000Batch:11 Sample:1100Batch:12 Sample:1200Batch:13 Sample:1300Batch:14 Sample:1400Batch:15 Sample:1500Batch:16 Sample:1600Batch:17 Sample:1700Batch:18 Sample:1800Batch:19 Sample:1900Batch:20 Sample:2000Batch:21 Sample:2100Batch:22 Sample:2200Batch:23 Sample:2300Batch:24 Sample:2400Batch:25 Sample:2500Batch:26 Sample:2600Batch:27 Sample:2700Batch:28 Sample:2800Batch:29 Sample:2900Batch:30 Sample:3000Batch:31 Sample:3100Batch:32 Sample:3200Batch:33 Sample:3300Batch:34 Sample:3400Batch:35 Sample:3500Batch:36 Sample:3600Batch:37 Sample:3700Batch:38 Sample:3800Batch:39 Sample:3900Batch:40 Sample:4000Batch:41 Sample:4100Batch:42 Sample:4200Batch:43 Sample:4300Batch:44 Sample:4400Batch:45 Sample:4500Batch:46 Sample:4600Batch:47 Sample:4700Batch:48 Sample:4800Batch:49 Sample:4900Batch:50 Sample:5000Batch:51 Sample:5100Batch:52 Sample:5200Batch:53 Sample:5300Batch:54 Sample:5400Traceback (most recent call last):
  File "Features-Extract.py", line 52, in <module>
    sample_count='all', batch_size=100, target_size=(224,224))
  File "Features-Extract.py", line 40, in extract_features
    h5_file.create_dataset('features-'+ str(batch_number), data=features_batch)
  File "/usr/local/lib/python2.7/dist-packages/h5py/_hl/group.py", line 106, in create_dataset
    dsid = dataset.make_new_dset(self, shape, dtype, data, **kwds)
  File "/usr/local/lib/python2.7/dist-packages/h5py/_hl/dataset.py", line 143, in make_new_dset
    dset_id.write(h5s.ALL, h5s.ALL, data)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5d.pyx", line 221, in h5py.h5d.DatasetID.write
  File "h5py/_proxy.pyx", line 132, in h5py._proxy.dset_rw
  File "h5py/_proxy.pyx", line 93, in h5py._proxy.H5PY_H5Dwrite
IOError: Can't prepare for writing data (file write failed: time = Tue Sep 11 22:45:05 2018
, filename = '/home/sivamurugan/deep-learning-models-master/VGG16/VGG16/train.h5', file descriptor = 3, errno = 28, error message = 'No space left on device', buf = 0x5fa3710, total write size = 10034400, bytes this sub-write = 10034400, bytes actually written = 18446744073709551615, offset = 542031872)
]0;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master/VGG16/VGG16[01;32msivamurugan@sivamurugan-PC[00m:[01;34m~/deep-learning-models-master/VGG16/VGG16[00m$ [K]0;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master/VGG16/VGG16[01;32msivamurugan@sivamurugan-PC[00m:[01;34m~/deep-learning-models-master/VGG16/VGG16[00m$ C[Kcd ../..
]0;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master[01;32msivamurugan@sivamurugan-PC[00m:[01;34m~/deep-learning-models-master[00m$ ls
[0m[01;34m224_OCT2017[0m  299.py           confusionmatrix.py  [01;34mINCEPTION[0m        microaneurysm1.py  PreprocessingCode.py  sriraman.pem     [01;34mVGG19[0m
224.py       96.py            [01;34mDENSENET[0m            [01;34mINCEPTIONRESNET[0m  microaneurysm.py   [01;34mRESNET50[0m              trainLabels.csv  [01;34mXCEPTION[0m
[01;34m299_OCT2017[0m  bloodvessels.py  exudates.py         [01;34mkeras-2.0.0[0m      [01;34mMOBILENET[0m          sivaraman.pem         [01;34mVGG16[0m
]0;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master[01;32msivamurugan@sivamurugan-PC[00m:[01;34m~/deep-learning-models-master[00m$ vi PreprocessingCode.py 
[?1049h[?1h=[2;1Hâ–½[6n[2;1H  [1;1H]11;?[1;43r[?12;25h[?12l[?25h[27m[23m[m[H[2J[?25l[43;1H"PreprocessingCode.py" [dos] 44L, 1481C[>c[27m[23m[m[H[2J[1;1H[38;5;81mimport[m glob
[38;5;81mimport[m cv2
[38;5;81mimport[m numpy [93mas[m np
[38;5;81mimport[m os
[38;5;81mfrom[m skimage.io [38;5;81mimport[m imread[11;1Hi = [95m1[m

[93mfor[m root, dirs, files [93min[m os.walk([95m"/home/sivamurugan/deep-learning-models-master/DATASET_299/test/"[m):
   [93mfor[m [1m[96mfile[m [93min[m files:[15;7H[93mif[m [1m[96mfile[m.endswith([95m'.jpeg'[m):[16;10Himgpath = os.path.join(root, [1m[96mfile[m)[17;10Himg = cv2.imread(imgpath)[18;10H[93mif[m img.size != [95m0[m:[19;13Hgrey = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)[20;13H_,th2 = cv2.threshold(grey,[95m8[m,[95m255[m,cv2.THRESH_BINARY)[21;13Hcontours, hierarchy = cv2.findContours(th2,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)[23;13Hareas = [cv2.contourArea(contour) [93mfor[m contour [93min[m contours][24;13Hmax_index = np.argmax(areas)[25;13Hcnt = contours[max_index][26;13Hx,y,w,h = cv2.boundingRect(cnt)[28;13H[96m# Ensure bounding rect should be at least 16:9 or taller[m[29;13H[93mif[m w / h > [95m16[m / [95m9[m:[30;16H[96m# increase top and bottom margin[m[31;16HnewHeight = w / [95m16[m * [95m9[m[32;16Hy = y - (newHeight - h ) / [95m2[m[33;16Hh = newHeight[34;13H[96m# Crop with the largest rectangle[m[35;13Hcrop = img[[1m[96mint[m(y):[1m[96mint[m(y+h),[1m[96mint[m(x):[1m[96mint[m(x+w)][36;13Hresized_img = cv2.resize(crop,([95m299[m, [95m299[m))[37;13H[1m[96mprint[m(imgpath)[38;13Hcv2.imshow([95m" Resized 224*224 Image"[m, resized_img)[39;13Hcv2.imwrite(os.path.join(imgpath),resized_img)[40;13Hcv2.waitKey([95m10[m)[41;13H[1m[96mprint[m([1m[96mstr[m(i))[42;13Hi = i+[95m1[m[43;126H1,1[11CTop"PreprocessingCode.py" [dos] 44L, 1481C[86C   [11C   [1;1H]2;PreprocessingCode.py (~/deep-learning-models-master) - VIM]1;PreprocessingCode.py[?12l[?25hP+q436f\P+q6b75\P+q6b64\P+q6b72\P+q6b6c\P+q2332\P+q2334\P+q2569\P+q2a37\P+q6b31\[?25l[43;1H[1m-- INSERT --[m[43;13H[K[43;126H1,1[11CTop[1;1H[?12l[?25h[?25l[43;126H2[2;1H[?12l[?25h[?25l[43;126H3[3;1H[?12l[?25h[?25l[43;126H4[4;1H[?12l[?25h[?25l[43;126H5[5;1H[?12l[?25h[?25l[43;126H6[6;1H[?12l[?25h[?25l[43;126H7[7;1H[?12l[?25h[?25l[43;126H8[8;1H[?12l[?25h[?25l[43;126H9[9;1H[?12l[?25h[?25l[43;126H10,1[10;1H[?12l[?25h[?25l[43;127H1[11;1H[?12l[?25h[?25l[43;127H2[12;1H[?12l[?25h[?25l[43;127H3[13;1H[?12l[?25h[?25l[43;129H2[13;2H[?12l[?25h[?25l[43;129H3[13;3H[?12l[?25h[?25l[43;129H4[13;4H[?12l[?25h[?25l[43;129H5[13;5H[?12l[?25h[?25l[43;129H6[13;6H[?12l[?25h[?25l[43;129H7[13;7H[?12l[?25h[?25l[43;129H8[13;8H[?12l[?25h[?25l[43;129H9[13;9H[?12l[?25h[?25l[43;129H10[13;10H[?12l[?25h[?25l[43;130H1[13;11H[?12l[?25h[?25l[43;130H2[13;12H[?12l[?25h[?25l[43;130H3[13;13H[?12l[?25h[?25l[43;130H4[13;14H[?12l[?25h[?25l[43;130H5[13;15H[?12l[?25h[?25l[43;130H6[13;16H[?12l[?25h[?25l[43;130H7[13;17H[?12l[?25h[?25l[43;130H8[13;18H[?12l[?25h[?25l[43;130H9[13;19H[?12l[?25h[?25l[43;129H20[13;20H[?12l[?25h[?25l[43;130H1[13;21H[?12l[?25h[?25l[43;130H2[13;22H[?12l[?25h[?25l[43;130H3[13;23H[?12l[?25h[?25l[43;130H4[13;24H[?12l[?25h[?25l[43;130H5[13;25H[?12l[?25h[?25l[43;130H6[13;26H[?12l[?25h[?25l[43;130H7[13;27H[?12l[?25h[?25l[43;130H8[13;28H[?12l[?25h[?25l[43;130H9[13;29H[?12l[?25h[?25l[43;129H30[13;30H[?12l[?25h[?25l[43;130H1[13;31H[?12l[?25h[?25l[43;130H2[13;32H[?12l[?25h[?25lk[46m([65C)[m[43;130H3[13;33H[?12l[?25h[?25l[43;130H4[13;34H[?12l[?25h[?25l([65C)[43;130H5[13;35H[?12l[?25h[?25l[43;130H6[13;36H[?12l[?25h[?25l[43;130H7[13;37H[?12l[?25h[?25l[43;130H8[13;38H[?12l[?25h[?25l[43;130H9[13;39H[?12l[?25h[?25l[43;129H40[13;40H[?12l[?25h[?25l[43;130H1[13;41H[?12l[?25h[?25l[43;130H2[13;42H[?12l[?25h[?25l[43;130H3[13;43H[?12l[?25h[?25l[43;130H4[13;44H[?12l[?25h[?25l[43;130H5[13;45H[?12l[?25h[?25l[43;130H6[13;46H[?12l[?25h[?25l[43;130H7[13;47H[?12l[?25h[?25l[43;130H8[13;48H[?12l[?25h[?25l[43;130H9[13;49H[?12l[?25h[?25l[43;129H50[13;50H[?12l[?25h[?25l[43;130H1[13;51H[?12l[?25h[?25l[43;130H2[13;52H[?12l[?25h[?25l[43;130H3[13;53H[?12l[?25h[?25l[95meep-learning-models-master/DATASET_299/test/"[m):[13;100H[K]2;PreprocessingCode.py + (~/deep-learning-models-master) - VIM]1;PreprocessingCode.py[13;53H[?12l[?25h[?25l[1C[95mp-learning-models-master/DATASET_299/test/"[m):[13;99H[K[13;53H[?12l[?25h[?25l[95mp-learning-models-master/DATASET_299/test/"[m):[13;98H[K[13;53H[?12l[?25h[?25l[95m-learning-models-master/DATASET_299/test/"[m):[13;97H[K[13;53H[?12l[?25h[?25l[95mlearning-models-master/DATASET_299/test/"[m):[13;96H[K[13;53H[?12l[?25h[?25l[95mearning-models-master/DATASET_299/test/"[m):[13;95H[K[13;53H[?12l[?25h[?25l[95marning-models-master/DATASET_299/test/"[m):[13;94H[K[13;53H[?12l[?25h[?25l[95mrning-models-master/DATASET_299/test/"[m):[13;93H[K[13;53H[?12l[?25h[?25l[95mning-models-master/DATASET_299/test/"[m):[13;92H[K[13;53H[?12l[?25h[?25l[95ming-models-master/DATASET_299/test/"[m):[13;91H[K[13;53H[?12l[?25h[?25l[95mng-models-master/DATASET_299/test/"[m):[13;90H[K[13;53H[?12l[?25h[?25l[95mg-models-master/DATASET_299/test/"[m):[13;89H[K[13;53H[?12l[?25h[?25l[95m-models-master/DATASET_299/test/"[m):[13;88H[K[13;53H[?12l[?25h[?25l[95mmodels-master/DATASET_299/test/"[m):[13;87H[K[13;53H[?12l[?25h[?25l[95models-master/DATASET_299/test/"[m):[13;86H[K[13;53H[?12l[?25h[?25l[95mdels-master/DATASET_299/test/"[m):[13;85H[K[13;53H[?12l[?25h[?25l[95mels-master/DATASET_299/test/"[m):[13;84H[K[13;53H[?12l[?25h[?25l[95mls-master/DATASET_299/test/"[m):[13;83H[K[13;53H[?12l[?25h[?25l[95ms-master/DATASET_299/test/"[m):[13;82H[K[13;53H[?12l[?25h[?25l[95m-master/DATASET_299/test/"[m):[13;81H[K[13;53H[?12l[?25h[?25l[95mmaster/DATASET_299/test/"[m):[13;80H[K[13;53H[?12l[?25h[?25l[95master/DATASET_299/test/"[m):[13;79H[K[13;53H[?12l[?25h[?25l[95mster/DATASET_299/test/"[m):[13;78H[K[13;53H[?12l[?25h[?25l[95mter/DATASET_299/test/"[m):[13;77H[K[13;53H[?12l[?25h[?25l[95mer/DATASET_299/test/"[m):[13;76H[K[13;53H[?12l[?25h[?25l[95mr/DATASET_299/test/"[m):[13;75H[K[13;53H[?12l[?25h[?25l[95m/DATASET_299/test/"[m):[13;74H[K[13;53H[?12l[?25h[?25l[95mD/DATASET_299/test/"[m):[43;130H4[13;54H[?12l[?25h[?25l[95mo/DATASET_299/test/"[m):[43;130H5[13;55H[?12l[?25h[?25l[95mc/DATASET_299/test/"[m):[43;130H6[13;56H[?12l[?25h[?25l[95mu/DATASET_299/test/"[m):[43;130H7[13;57H[?12l[?25h[?25l[95mm/DATASET_299/test/"[m):[43;130H8[13;58H[?12l[?25h[?25l[95me/DATASET_299/test/"[m):[43;130H9[13;59H[?12l[?25h[?25l[95mn/DATASET_299/test/"[m):[43;129H60[13;60H[?12l[?25h[?25l[95mt/DATASET_299/test/"[m):[43;130H1[13;61H[?12l[?25h[?25l[95ms/DATASET_299/test/"[m):[43;130H2[13;62H[?12l[?25h[?25l[43;130H3[13;63H[?12l[?25h[?25l[43;130H4[13;64H[?12l[?25h[?25l[95mRATASET_299/test/"[m):[43;130H5[13;65H[?12l[?25h[?25l[95mTASET_299/test/"[m):[13;83H[K[13;65H[?12l[?25h[?25l[95mASET_299/test/"[m):[13;82H[K[13;65H[?12l[?25h[?25l[95mSET_299/test/"[m):[13;81H[K[13;65H[?12l[?25h[?25l[95mET_299/test/"[m):[13;80H[K[13;65H[?12l[?25h[?25l[95mT_299/test/"[m):[13;79H[K[13;65H[?12l[?25h[?25l[95m_299/test/"[m):[13;78H[K[13;65H[?12l[?25h[?25l[43;130H6[13;66H[?12l[?25h[?25l[43;130H7[13;67H[?12l[?25h[?25l[43;130H8[13;68H[?12l[?25h[?25l[43;130H9[13;69H[?12l[?25h[?25l[43;129H70[13;70H[?12l[?25h[?25l[43;130H1[13;71H[?12l[?25h[?25l[95mrest/"[m):[43;130H2[13;72H[?12l[?25h[?25l[95maest/"[m):[43;130H3[13;73H[?12l[?25h[?25l[95miest/"[m):[43;130H4[13;74H[?12l[?25h[?25l[95mnest/"[m):[43;130H5[13;75H[?12l[?25h[?25l[95mst/"[m):[13;81H[K[13;75H[?12l[?25h[?25l[95mt/"[m):[13;80H[K[13;75H[?12l[?25h[?25l[95m/"[m):[13;79H[K[13;75H[?12l[?25h[?25l[43;127H4,22[14;22H[?12l[?25h[?25l[43;127H5,36[15;36H[?12l[?25h[?25l[16;32H[46m([10C)[m[43;127H6,44[16;44H[?12l[?25h[?25l[16;32H([10C)[17;26H[46m([7C)[m[43;127H7,35[17;35H[?12l[?25h[?25l[17;26H([7C)[43;127H8,29-33[18;33H[?12l[?25h[?25l[19;32H[46m([22C)[m[43;127H9,56   [19;56H[?12l[?25h[?25l[19;32H([22C)[20;34H[46m([28C)[m[43;126H20,64[20;64H[?12l[?25h[?25l[20;34H([28C)[43;127H1,75[21;75H[?12l[?25h[?25l[43;127H2,4 [22;4H[?12l[?25h[?25l[23;21H[46m[[48C][m[43;127H3,71[23;71H[?12l[?25h[?25l[23;21H[[48C][24;34H[46m([mareas[46m)[m[43;127H4,4[24;41H[?12l[?25h[?25l[24;34H(areas)[25;27H[46m[[9C][m[43;127H5,38[25;38H[?12l[?25h[?25l[25;27H[[9C][26;39H[46m([mcnt[46m)[m[43;127H6,44[26;44H[?12l[?25h[?25l(cnt)[43;127H7,1 [27;1H[?12l[?25h[?25l[43;127H8,69[28;69H[?12l[?25h[?25l[43;127H9,31[29;31H[?12l[?25h[?25l[43;126H30,48[30;48H[?12l[?25h[?25l[43;127H1,3[31;38H[?12l[?25h[?25l[43;127H2,44[32;44H[?12l[?25h[?25l[43;127H3,29[33;29H[?12l[?25h[?25l[43;127H4,46[34;46H[?12l[?25h[?25l[43;127H5,57[35;57H[?12l[?25h[?25l[36;37H[46m([15C)[m[43;127H6,54[36;54H[?12l[?25h[?25l[36;37H([15C)[37;18H[46m([7C)[m[43;127H7,27[37;27H[?12l[?25h[?25l[37;18H([7C)[38;23H[46m([37C)[m[43;127H8,62[38;62H[?12l[?25h[?25l[43;130H1[38;61H[?12l[?25h[?25l[38;23H([37C)[43;130H0[38;60H[?12l[?25h[?25l[43;129H59[38;59H[?12l[?25h[?25l[43;130H8[38;58H[?12l[?25h[?25l[43;130H7[38;57H[?12l[?25h[?25l[43;130H6[38;56H[?12l[?25h[?25l[43;130H5[38;55H[?12l[?25h[?25l[43;130H4[38;54H[?12l[?25h[?25l[43;130H3[38;53H[?12l[?25h[?25l[43;130H2[38;52H[?12l[?25h[?25l[43;130H1[38;51H[?12l[?25h[?25l[43;130H0[38;50H[?12l[?25h[?25l[43;129H49[38;49H[?12l[?25h[?25l[43;130H8[38;48H[?12l[?25h[?25l[43;130H7[38;47H[?12l[?25h[?25l[43;130H6[38;46H[?12l[?25h[?25l[43;130H5[38;45H[?12l[?25h[?25l[43;130H4[38;44H[?12l[?25h[?25l[43;130H3[38;43H[?12l[?25h[?25l[43;130H2[38;42H[?12l[?25h[?25l[43;130H1[38;41H[?12l[?25h[?25l[43;130H0[38;40H[?12l[?25h[?25l[43;129H39[38;39H[?12l[?25h[?25l[43;130H8[38;38H[?12l[?25h[?25l[43;130H7[38;37H[?12l[?25h[?25l[43;130H6[38;36H[?12l[?25h[?25l[43;130H5[38;35H[?12l[?25h[?25l[95m924*224 Image"[m, resized_img)[43;130H6[38;36H[?12l[?25h[?25l[95m924*224 Image"[m, resized_img)[43;130H7[38;37H[?12l[?25h[?25l[95m4*224 Image"[m, resized_img)[38;63H[K[38;37H[?12l[?25h[?25l[95m*224 Image"[m, resized_img)[38;62H[K[38;37H[?12l[?25h[?25l[43;130H8[38;38H[?12l[?25h[?25l[43;130H9[38;39H[?12l[?25h[?25l[95m924 Image"[m, resized_img)[43;129H40[38;40H[?12l[?25h[?25l[95m924 Image"[m, resized_img)[43;130H1[38;41H[?12l[?25h[?25l[95m4 Image"[m, resized_img)[38;63H[K[38;41H[?12l[?25h[?25l[95m Image"[m, resized_img)[38;62H[K[38;41H[?12l[?25h[?25l[43;127H9[39;41H[?12l[?25h[?25l[43;126H40,30[40;30H[?12l[?25h[?25l[41;18H[46m([6C)[m[43;127H1,26[41;26H[?12l[?25h[?25l[41;18H([6C)[43;127H2,31-41[42;41H[?12l[?25h[?25l[1;42r[42;1H
[1;43r[43;126H[K[43;126H43,1[10C50%[42;1H[?12l[?25h[?25l[1;42r[42;1H
[1;43r[43;126H[K[43;126H44,1[10CBot[42;1H[?12l[?25h[43;1H[K[42;1H[?25l[43;126H44,0-1[8CBot[42;1H[?12l[?25h[?25l[43;126H[K[43;1H:[?12l[?25hw[?25l[?12l[?25hq[?25l[?12l[?25h[?25l"PreprocessingCode.py" [dos] 44L, 1459C written]2;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master]1;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master
[?1l>[?12l[?25h[?1049l]0;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master[01;32msivamurugan@sivamurugan-PC[00m:[01;34m~/deep-learning-models-master[00m$ vi PreprocessingCode.py [1P[1P[1@p[1@y[1@t[1@h[1@o[1@n[1@2[C[1@ 
]0;sivamurugan@sivamurugan-PC: ~/deep-learning-models-master[01;32msivamurugan@sivamurugan-PC[00m:[01;34m~/deep-learning-models-master[00m$ python2  PreprocessingCode.py [6Pvi[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C
[?1049h[?1h=[2;1Hâ–½[6n[2;1H  [1;1H]11;?[1;43r[?12;25h[?12l[?25h[27m[23m[m[H[2J[?25l[43;1H"PreprocessingCode.py" [dos] 44L, 1459C[>c[27m[23m[m[H[2J[1;1H[38;5;81mimport[m glob
[38;5;81mimport[m cv2
[38;5;81mimport[m numpy [93mas[m np
[38;5;81mimport[m os
[38;5;81mfrom[m skimage.io [38;5;81mimport[m imread[11;1Hi = [95m1[m

[93mfor[m root, dirs, files [93min[m os.walk([95m"/home/sivamurugan/Documents/DR_299/train/"[m):
   [93mfor[m [1m[96mfile[m [93min[m files:[15;7H[93mif[m [1m[96mfile[m.endswith([95m'.jpeg'[m):[16;10Himgpath = os.path.join(root, [1m[96mfile[m)[17;10Himg = cv2.imread(imgpath)[18;10H[93mif[m img.size != [95m0[m:[19;13Hgrey = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)[20;13H_,th2 = cv2.threshold(grey,[95m8[m,[95m255[m,cv2.THRESH_BINARY)[21;13Hcontours, hierarchy = cv2.findContours(th2,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)[23;13Hareas = [cv2.contourArea(contour) [93mfor[m contour [93min[m contours][24;13Hmax_index = np.argmax(areas)[25;13Hcnt = contours[max_index][26;13Hx,y,w,h = cv2.boundingRect(cnt)[28;13H[96m# Ensure bounding rect should be at least 16:9 or taller[m[29;13H[93mif[m w / h > [95m16[m / [95m9[m:[30;16H[96m# increase top and bottom margin[m[31;16HnewHeight = w / [95m16[m * [95m9[m[32;16Hy = y - (newHeight - h ) / [95m2[m[33;16Hh = newHeight[34;13H[96m# Crop with the largest rectangle[m[35;13Hcrop = img[[1m[96mint[m(y):[1m[96mint[m(y+h),[1m[96mint[m(x):[1m[96mint[m(x+w)][36;13Hresized_img = cv2.resize(crop,([95m299[m, [95m299[m))[37;13H[1m[96mprint[m(imgpath)[38;13Hcv2.imshow([95m" Resized 299*299 Image"[m, resized_img)[39;13Hcv2.imwrite(os.path.join(imgpath),resized_img)[40;13Hcv2.waitKey([95m10[m)[41;13H[1m[96mprint[m([1m[96mstr[m(i))[42;13Hi = i+[95m1[m[43;126H1,1[11CTop"PreprocessingCode.py" [dos] 44L, 1459C[86C   [11C   [1;1HP+q436f\P+q6b75\P+q6b64\P+q6b72\P+q6b6c\P+q2332\P+q2334\P+q2569\P+q2a37\P+q6b31\]2;PreprocessingCode.py (~/deep-learning-models-master) - VIM]1;PreprocessingCode.py[?12l[?25h[?25l[43;126H2,1[11CTop[2;1H[?12l[?25h[?25l[43;126H3[3;1H[?12l[?25h[?25l[43;126H4[4;1H[?12l[?25h[?25l[43;126H5[5;1H[?12l[?25h[?25l[43;126H6,0-1[6;1H[?12l[?25h[?25l[43;126H7[7;1H[?12l[?25h[?25l[43;126H8[8;1H[?12l[?25h[?25l[43;126H9[9;1H[?12l[?25h[?25l[43;126H10,0-1[10;1H[?12l[?25h[?25l[43;127H1,1  [11;1H[?12l[?25h[?25l[43;127H2,0-1[12;1H[?12l[?25h[?25l[43;127H3,1  [13;1H[?12l[?25h[?25l[43;129H2[13;2H[?12l[?25h[?25l[43;129H3[13;3H[?12l[?25h[?25l[43;129H4[13;4H[?12l[?25h[?25l[43;129H5[13;5H[?12l[?25h[?25l[43;129H6[13;6H[?12l[?25h[?25l[43;129H7[13;7H[?12l[?25h[?25l[43;129H8[13;8H[?12l[?25h[?25l[43;129H9[13;9H[?12l[?25h[?25l[43;129H10[13;10H[?12l[?25h[?25l[43;130H1[13;11H[?12l[?25h[?25l[43;130H2[13;12H[?12l[?25h[?25l[43;130H3[13;13H[?12l[?25h[?25l[43;130H4[13;14H[?12l[?25h[?25l[43;130H5[13;15H[?12l[?25h[?25l[43;130H6[13;16H[?12l[?25h[?25l[43;130H7[13;17H[?12l[?25h[?25l[43;130H8[13;18H[?12l[?25h[?25l[43;130H9[13;19H[?12l[?25h[?25l[43;129H20[13;20H[?12